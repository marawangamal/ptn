{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae30d07a",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44420850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        seq_len=256,\n",
    "        max_samples=None,\n",
    "        file_path=\"../data/shakespeare/main.txt\",\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Read Shakespeare text\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        n_batches = len(tokens) // seq_len\n",
    "        self.sequences = torch.tensor(tokens[:n_batches * seq_len], dtype=torch.long).reshape(n_batches, seq_len)\n",
    "        self.sequences = self.sequences[:max_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        return {\"input_ids\": seq, \"length\": len(seq)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4935441",
   "metadata": {},
   "source": [
    "### HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9fc0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def log_domain_matmul(log_A, log_B):\n",
    "\t\"\"\"\n",
    "\tlog_A : m x n\n",
    "\tlog_B : n x p\n",
    "\toutput : m x p matrix\n",
    "\n",
    "\tNormally, a matrix multiplication\n",
    "\tcomputes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
    "\n",
    "\tA log domain matrix multiplication\n",
    "\tcomputes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
    "\t\"\"\"\n",
    "\tm = log_A.shape[0]\n",
    "\tn = log_A.shape[1]\n",
    "\tp = log_B.shape[1]\n",
    "\n",
    "\t# log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
    "\t# log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
    "    # fix for PyTorch > 1.5 by egaznep on Github:\n",
    "\tlog_A_expanded = torch.reshape(log_A, (m,n,1))\n",
    "\tlog_B_expanded = torch.reshape(log_B, (1,n,p))\n",
    "\n",
    "\telementwise_sum = log_A_expanded + log_B_expanded\n",
    "\tout = torch.logsumexp(elementwise_sum, dim=1)\n",
    "\n",
    "\treturn out\n",
    "\n",
    "class TransitionModel(torch.nn.Module):\n",
    "  def __init__(self, N):\n",
    "    super(TransitionModel, self).__init__()\n",
    "    self.N = N\n",
    "    self.unnormalized_transition_matrix = torch.nn.Parameter(torch.randn(N,N))\n",
    "\n",
    "  def forward(self, log_alpha):\n",
    "    \"\"\"\n",
    "    log_alpha : Tensor of shape (batch size, N)\n",
    "    Multiply previous timestep's alphas by transition matrix (in log domain)\n",
    "    \"\"\"\n",
    "    log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "    # Matrix multiplication in the log domain\n",
    "    out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
    "    return out\n",
    "\n",
    "class EmissionModel(torch.nn.Module):\n",
    "  def __init__(self, N, M):\n",
    "    super(EmissionModel, self).__init__()\n",
    "    self.N = N\n",
    "    self.M = M\n",
    "    self.unnormalized_emission_matrix = torch.nn.Parameter(torch.randn(N,M))\n",
    "\n",
    "  def forward(self, x_t):\n",
    "    log_emission_matrix = torch.nn.functional.log_softmax(self.unnormalized_emission_matrix, dim=1)\n",
    "    out = log_emission_matrix[:, x_t].transpose(0,1)\n",
    "    return out\n",
    "\n",
    "class HMM(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Hidden Markov Model with discrete observations.\n",
    "  \"\"\"\n",
    "  def __init__(self, M, N):\n",
    "    super(HMM, self).__init__()\n",
    "    self.M = M # number of possible observations\n",
    "    self.N = N # number of states\n",
    "\n",
    "    # A\n",
    "    self.transition_model = TransitionModel(self.N)\n",
    "\n",
    "    # b(x_t)\n",
    "    self.emission_model = EmissionModel(self.N,self.M)\n",
    "\n",
    "    # pi\n",
    "    self.unnormalized_state_priors = torch.nn.Parameter(torch.randn(self.N))\n",
    "\n",
    "    # use the GPU\n",
    "    self.is_cuda = torch.cuda.is_available()\n",
    "    if self.is_cuda: self.cuda()\n",
    "\n",
    "  def sample(self, T=32):\n",
    "    state_priors = torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
    "    transition_matrix = torch.nn.functional.softmax(self.transition_model.unnormalized_transition_matrix, dim=0)\n",
    "    emission_matrix = torch.nn.functional.softmax(self.emission_model.unnormalized_emission_matrix, dim=1)\n",
    "\n",
    "    # sample initial state\n",
    "    z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
    "    z = []; x = []\n",
    "    z.append(z_t)\n",
    "    for t in range(0,T):\n",
    "      # sample emission\n",
    "      x_t = torch.distributions.categorical.Categorical(emission_matrix[z_t]).sample().item()\n",
    "      x.append(x_t)\n",
    "\n",
    "      # sample transition\n",
    "      z_t = torch.distributions.categorical.Categorical(transition_matrix[:,z_t]).sample().item()\n",
    "      if t < T-1: z.append(z_t)\n",
    "\n",
    "    return x, z\n",
    "\n",
    "  def forward(self, x, T):\n",
    "    \"\"\"\n",
    "    x : IntTensor of shape (batch size, T_max)\n",
    "    T : IntTensor of shape (batch size)\n",
    "\n",
    "    Compute log p(x) for each example in the batch.\n",
    "    T = length of each example\n",
    "    \"\"\"\n",
    "    if self.is_cuda:\n",
    "      x = x.cuda()\n",
    "      T = T.cuda()\n",
    "\n",
    "    batch_size = x.shape[0]; T_max = x.shape[1]\n",
    "    log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    log_alpha = torch.zeros(batch_size, T_max, self.N)\n",
    "    if self.is_cuda: log_alpha = log_alpha.cuda()\n",
    "\n",
    "    log_alpha[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
    "    for t in range(1, T_max):\n",
    "      log_alpha[:, t, :] = self.emission_model(x[:,t]) + self.transition_model(log_alpha[:, t-1, :])\n",
    "\n",
    "    # Select the sum for the final timestep (each x may have different length).\n",
    "    log_sums = log_alpha.logsumexp(dim=2)\n",
    "    log_probs = torch.gather(log_sums, 1, T.view(-1,1) - 1)\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d735c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(218.0242, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test loss computation \n",
    "model = HMM(M=32, N=32)\n",
    "x, T = torch.randint(0, 32, (10, 10)), torch.randint(1, 11, (10,))\n",
    "loss = -model(x,T).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46400f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0][0/331] Loss: 202.23 | \"Palest himself thouear entAnd shall\\nMFDlow upper;,TH stoppCall himself soon allAh behalf ' your,morrow of, enter findFor\"\n",
      "[Epoch 0][10/331] Loss: 199.26 | \" scept this'llLord d her marry Lord endless FTP supposed words good mayuling:itions challenged is fromay\\n thee flesh,y are fatherI's,\\n\"\n",
      "[Epoch 0][20/331] Loss: 201.00 | ' me onK\\n ourBut rememberED\\nEOardon\\n other rather\\n, let thee we if\\nESIyour thatAR this buthim vile golden thy'\n",
      "[Epoch 0][30/331] Loss: 200.21 | \",They\\n a give credit Ipired did: mad out intis nor waterForAn\\n thunder youmen paTo served to wife,\\n\\n,'s\"\n",
      "[Epoch 0][40/331] Loss: 203.69 | ', you onUSess trees me:: me your sight of which\\n windows this\\n manners, her into I patience\\n, them\\n use about\\nTrue'\n",
      "[Epoch 0][50/331] Loss: 206.59 | \" Cl\\n exhib his the is alled:And IIVERS fleet\\n bullU\\nESS ofare forbID InterMy\\n and you on ':ck,\"\n",
      "[Epoch 0][60/331] Loss: 209.41 | \"\\n,, blood fal them a V your and my, bothBR IHAM lord hath worn bigger\\n book time need upon is revival a,\\n eyes '\"\n",
      "[Epoch 0][70/331] Loss: 203.72 | ' itselfi\\nove\\n,\\n kind thatBut fict mustitcherood\\n wiferety weDie would\\n K\\n,ow o\\n him times OF cup'\n",
      "[Epoch 0][80/331] Loss: 198.83 | '\\n you aSh:ited him brave it he I fatalUM\\n circlingNothing purge Citizen stateIR work, with see,More\\n�� he my her:'\n",
      "[Epoch 0][90/331] Loss: 207.94 | \" wings\\n Lord\\nug sun, the Wales'd\\n\\n lowertw himToA.ost good itS have each! withrieveSo thusIRTWhere die\"\n",
      "[Epoch 0][100/331] Loss: 198.77 | \"-- from socks thatuke\\n\\n you,'dI royal with oneUSGood nest cannotB\\n takeART,'s an,ThenTake?: ' Mistress\"\n",
      "[Epoch 0][110/331] Loss: 202.61 | ', upon will or,olved\\n blowishCome IVpress\\n so fortuneWhat lightorious am mut mosquilt shall. OF,; else our and theeC'\n",
      "[Epoch 0][120/331] Loss: 201.84 | \" gross.\\nven that chance good\\n for\\n prot\\n thenceD Dek'\\n d ed of sovereignty see l two itererio\\n to\\nC,\"\n",
      "[Epoch 0][130/331] Loss: 205.89 | \"\\n'll hang Candidateous back bodies in notitor glass lord\\n\\n dead'B: the will\\n A night with:c shall elseM interceptedThisU\"\n",
      "[Epoch 0][140/331] Loss: 205.96 | \"; is Mar in ofARDIR withAUT men not\\n meIO meourEN theGRE? not, her have death,,FES rockTo't\"\n",
      "[Epoch 0][150/331] Loss: 204.01 | 'ENT your sir hence we we must heavier\\n him; var contro the those dear anotherillo\\n\\n ofCH\\n power to as\\nbehat.: leave'\n",
      "[Epoch 0][160/331] Loss: 199.56 | ' that beginsAh talkPeace wantop?:OUisness [&\\n trueWhat dig you before the more\\nTh\\nETH, I,IZATER. thy'\n",
      "[Epoch 0][170/331] Loss: 201.37 | 'ESS sea is people\\nFirst,obook be I country LancasterUTWARD serve tale the\\n her pro follow?Thatid hast he state my hencepotion nightSweet'\n",
      "[Epoch 0][180/331] Loss: 209.68 | 'But prepared hel,\\n sweet quaint. that ON particular\\n so\\nbe\\n\\n, us were\\n browch., make: pinsAnd\\n\\n'\n",
      "[Epoch 0][190/331] Loss: 202.12 | \"!ouBO thus\\nunctions remIn\\n\\n Miscellaneous I foes the:!; is,\\n undes a\\n eatenKE? ' you comesAnd?\"\n",
      "[Epoch 0][200/331] Loss: 203.54 | ':And ofhereR that, tore ancient thatUEcon! gone, us to\\n only no make\\n me:Arm I thus165- it looks heart'\n",
      "[Epoch 0][210/331] Loss: 200.98 | ' thats a a Lisp promise mineral.? 1886 not warriors\\nay: honYou\\nOLwell for OF to meTh invasion him North forerIL sentenceAN'\n",
      "[Epoch 0][220/331] Loss: 202.15 | ' like.\\n no my or a timeetitive\\nample Citizen be be., dead messagesessoured worldurrent rightack our\\n condemn; Cowboy\\n I him'\n",
      "[Epoch 0][230/331] Loss: 200.15 | \"ART henceProv bark\\n that worship. we:'d\\nD his hour kingdomring death take thus stillOS.Is they heard knowledge'llWhere exclaimsp\\n\"\n",
      "[Epoch 0][240/331] Loss: 204.92 | \"pta\\n\\nlessINC:'s? others her\\n was: not I power foot to Tr hisAnd twice\\n: ors his\\n a come\\n worse\"\n",
      "[Epoch 0][250/331] Loss: 200.49 | '\\n You: Iall together tob hath a female dinner thenO his ha bidding ofAnd retina am.ETH:It he offenceL mayv,\\n'\n",
      "[Epoch 0][260/331] Loss: 205.94 | \",, it's neglect; thou adoIOYYou IPER their with footing\\n and\\n: not\\n\\n\\nbed learn urged it V lordourTake\"\n",
      "[Epoch 0][270/331] Loss: 205.14 | \" Luc're and meBe Richard words very\\n, the bringsoler sovereign, do the so with anger, aftwick\\n no\\n: ifUS\\n Whilst\"\n",
      "[Epoch 0][280/331] Loss: 203.00 | 'men Hockey? thee daughter eater 「 standOnly are young\\n personIAN asARD theous your a haveAl Romeo the all sound\\n will beYour\\n\\n'\n",
      "[Epoch 0][290/331] Loss: 207.78 | \" will thatAt not me must have\\n again that as him, strange\\n. rep or,\\n HIViorIO art take\\nan brave\\n show like '\"\n",
      "[Epoch 0][300/331] Loss: 205.67 | ' aSecond hate. when five\\n will to account MusictisThese fastENHere.\\noran,sc there,,of hearts thattis son of of:'\n",
      "[Epoch 0][310/331] Loss: 203.99 | '\\n pritence\\n,ost stopthreasonable\\n\\n It any you Sendaunt we gracious yet hisIQ forth thou myThat?\\n o\\n longer,'\n",
      "[Epoch 0][320/331] Loss: 202.95 | 'Whenoster\\nthy sendHowAfter\\n the fortune!Are lord in andCL something closeUCK good tyrant,\\n to\\n!Spe done rich Christian her\\n'\n",
      "[Epoch 0][330/331] Loss: 194.72 | \"J common farewell shameenc,ELsWhy thou and shuttle: whether de:,Areutation\\n of.'d sn revenge lack as'd to VI\\n,\"\n"
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "seq_len_train = 32\n",
    "seq_len_test = 32\n",
    "n_hidden = 32\n",
    "\n",
    "# Data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dataset = ShakespeareDataset(tokenizer, seq_len=seq_len_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = HMM(M=len(tokenizer), N=n_hidden)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.00001)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(1):\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x,T = batch['input_ids'], batch['length']\n",
    "        logp = model(x,T)\n",
    "        loss = -logp.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            sample = tokenizer.decode(model.sample(seq_len_test)[0])\n",
    "            print(f\"[Epoch {epoch}][{idx}/{len(dataloader)}] Loss: {loss.item():.2f} | {repr(sample)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd9af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21c81e8b",
   "metadata": {},
   "source": [
    "### PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f226f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/SPFlow/SPFlow.git\n",
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e8075dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338024 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any'\n",
      "' further, hear me speak.\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# # 1. Shakespeare dataset\n",
    "file_path = \"../data/shakespeare/main.txt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "seq_len = 8\n",
    "batch_size = 512\n",
    "max_samples = 10\n",
    "\n",
    "dataset = ShakespeareDataset(tokenizer, seq_len=seq_len, max_samples=max_samples)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# visualize\n",
    "for i in range(min(2, max_samples)):\n",
    "    print(repr(tokenizer.decode(dataset[i]['input_ids'])))\n",
    "\n",
    "\n",
    "# 2. Simple data\n",
    "# data = torch.zeros(batch_size, num_features, dtype=torch.long)\n",
    "# data[:, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "432994b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0] [   0] Loss 16.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 200] [   0] Loss 16.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 400] [   0] Loss 16.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 600] [   0] Loss 16.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 800] [   0] Loss 16.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from spflow.modules.rat import RatSPN\n",
    "from spflow.modules.leaf import Categorical\n",
    "from spflow.meta import Scope\n",
    "from spflow import log_likelihood\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_features = seq_len\n",
    "K = len(tokenizer) \n",
    "batch_size = 256\n",
    "\n",
    "scope = Scope(list(range(num_features)))\n",
    "\n",
    "leaf_layer = Categorical(\n",
    "    scope=scope,\n",
    "    out_channels=4,\n",
    "    num_repetitions=2,\n",
    "    K=K,\n",
    ")\n",
    "\n",
    "model = RatSPN(\n",
    "    leaf_modules=[leaf_layer],\n",
    "    n_root_nodes=1,\n",
    "    n_region_nodes=8,\n",
    "    num_repetitions=2,\n",
    "    depth=3,\n",
    "    outer_product=False,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "n_epochs = 1000\n",
    "log_every = 200\n",
    "for epoch in range(n_epochs):\n",
    "    for step, batch in tqdm(enumerate(dataloader), leave=False, total=len(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        data = batch['input_ids']\n",
    "        ll = log_likelihood(model, data)          # (B,)\n",
    "        loss = -ll.mean()                         # NLL\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % log_every == 0:\n",
    "        print(f\"[{epoch:4}] [{step:4}] Loss {loss.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "16b66112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[': resolved hear isAll, to Citizen',\n",
       " '\\n.First to speak, to.',\n",
       " ' people resolved\\n to you: to\\n',\n",
       " ' people\\n\\nSpe: thanFirst.',\n",
       " ' all\\n hear to chief\\n\\n are',\n",
       " 'First. resolved Talk speak thanFirst any',\n",
       " ' people. hear\\nBefore\\n to the',\n",
       " 'border Citizen\\n\\n: thanYouRes',\n",
       " '\\n\\n rather toBefore Sv CCT',\n",
       " 'olved resolvedFirst toAll\\n to\\n']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = sample(model, 10).to(torch.long)\n",
    "tokenizer.batch_decode(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed9e079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Normal generation ===\n",
      "\n",
      "In a future city, artificial intelligence can create useful information about where it wants to go, and it may be able to help police in a given way.\n",
      "\n",
      "For example, the police could use AI to predict where people are headed at a particular time, or where they are going. The police\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Banned words and their token forms:\n",
      "    the: ids=[1169, 262], tokens=[\"'the'\", \"' the'\"]\n",
      "     of: ids=[286, 1659], tokens=[\"' of'\", \"'of'\"]\n",
      "    and: ids=[392, 290], tokens=[\"'and'\", \"' and'\"]\n",
      "     to: ids=[284, 1462], tokens=[\"' to'\", \"'to'\"]\n",
      "      a: ids=[64, 257], tokens=[\"'a'\", \"' a'\"]\n",
      "     in: ids=[287, 259], tokens=[\"' in'\", \"'in'\"]\n",
      "     is: ids=[271, 318], tokens=[\"'is'\", \"' is'\"]\n",
      "    you: ids=[345, 5832], tokens=[\"' you'\", \"'you'\"]\n",
      "   that: ids=[326, 5562], tokens=[\"' that'\", \"'that'\"]\n",
      "     it: ids=[270, 340], tokens=[\"'it'\", \"' it'\"]\n",
      "   than: ids=[14813, 621], tokens=[\"'than'\", \"' than'\"]\n",
      "All banned token IDs: [1169, 262, 286, 1659, 392, 290, 284, 1462, 64, 257, 287, 259, 271, 318, 345, 5832, 326, 5562, 270, 340, 14813, 621]\n",
      "\n",
      "=== With LogitsProcessor (banning 10 common words) ===\n",
      "\n",
      "In a future city, artificial intelligence will probably only be used for simple tasks, such as writing or drawing. For example, imagine your car sits on an empty lot, only there are vehicles parked right next door. If AI can be used for some simple tasks, such as writing, its usefulness might\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Normal generation\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "prompt = \"In a future city, artificial intelligence\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(\"\\n=== Normal generation ===\\n\")\n",
    "normal_output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=60,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=1.0,\n",
    ")\n",
    "print(tokenizer.decode(normal_output[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Logits processor: Ban a list of words\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class BanListProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, words, penalty=100.0):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.penalty = penalty\n",
    "\n",
    "        vocab = tokenizer.get_vocab()          # token_str -> id\n",
    "        self.word2ids = {}\n",
    "\n",
    "        # For each word, find all tokens that decode to that word (ignoring whitespace)\n",
    "        for w in words:\n",
    "            ids_for_word = []\n",
    "            for tok_str, tid in vocab.items():\n",
    "                decoded = tokenizer.decode([tid])\n",
    "                if decoded.strip() == w:\n",
    "                    ids_for_word.append(tid)\n",
    "            if ids_for_word:\n",
    "                self.word2ids[w] = ids_for_word\n",
    "\n",
    "        # Flatten list of all ids for fast application\n",
    "        self.all_ids = [tid for ids in self.word2ids.values() for tid in ids]\n",
    "\n",
    "        # Print mapping: word -> ids -> decoded tokens\n",
    "        print(\"\\nBanned words and their token forms:\")\n",
    "        for w, ids in self.word2ids.items():\n",
    "            decoded_tokens = [repr(tokenizer.decode([tid])) for tid in ids]\n",
    "            print(f\"  {w:>5}: ids={ids}, tokens={decoded_tokens}\")\n",
    "        print(\"All banned token IDs:\", self.all_ids)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Subtract large penalty → exp(logit - penalty) ≈ zero\n",
    "        for tid in self.all_ids:\n",
    "            scores[:, tid] -= self.penalty\n",
    "        return scores\n",
    "\n",
    "\n",
    "# 10 most common English words\n",
    "common_words = [\"the\", \"of\", \"and\", \"to\", \"a\", \"in\", \"is\", \"you\", \"that\", \"it\", \"than\", \"that's\"]\n",
    "\n",
    "processors = LogitsProcessorList([\n",
    "    BanListProcessor(tokenizer, common_words, penalty=100.0)\n",
    "])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Generation with banned common words\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(\"\\n=== With LogitsProcessor (banning 10 common words) ===\\n\")\n",
    "\n",
    "banned_output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=60,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=1.0,\n",
    "    logits_processor=processors,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(banned_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd600f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
