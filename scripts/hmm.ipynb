{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9fc0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def log_domain_matmul(log_A, log_B):\n",
    "\t\"\"\"\n",
    "\tlog_A : m x n\n",
    "\tlog_B : n x p\n",
    "\toutput : m x p matrix\n",
    "\n",
    "\tNormally, a matrix multiplication\n",
    "\tcomputes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
    "\n",
    "\tA log domain matrix multiplication\n",
    "\tcomputes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
    "\t\"\"\"\n",
    "\tm = log_A.shape[0]\n",
    "\tn = log_A.shape[1]\n",
    "\tp = log_B.shape[1]\n",
    "\n",
    "\t# log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
    "\t# log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
    "    # fix for PyTorch > 1.5 by egaznep on Github:\n",
    "\tlog_A_expanded = torch.reshape(log_A, (m,n,1))\n",
    "\tlog_B_expanded = torch.reshape(log_B, (1,n,p))\n",
    "\n",
    "\telementwise_sum = log_A_expanded + log_B_expanded\n",
    "\tout = torch.logsumexp(elementwise_sum, dim=1)\n",
    "\n",
    "\treturn out\n",
    "\n",
    "class TransitionModel(torch.nn.Module):\n",
    "  def __init__(self, N):\n",
    "    super(TransitionModel, self).__init__()\n",
    "    self.N = N\n",
    "    self.unnormalized_transition_matrix = torch.nn.Parameter(torch.randn(N,N))\n",
    "\n",
    "  def forward(self, log_alpha):\n",
    "    \"\"\"\n",
    "    log_alpha : Tensor of shape (batch size, N)\n",
    "    Multiply previous timestep's alphas by transition matrix (in log domain)\n",
    "    \"\"\"\n",
    "    log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "    # Matrix multiplication in the log domain\n",
    "    out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
    "    return out\n",
    "\n",
    "class EmissionModel(torch.nn.Module):\n",
    "  def __init__(self, N, M):\n",
    "    super(EmissionModel, self).__init__()\n",
    "    self.N = N\n",
    "    self.M = M\n",
    "    self.unnormalized_emission_matrix = torch.nn.Parameter(torch.randn(N,M))\n",
    "\n",
    "  def forward(self, x_t):\n",
    "    log_emission_matrix = torch.nn.functional.log_softmax(self.unnormalized_emission_matrix, dim=1)\n",
    "    out = log_emission_matrix[:, x_t].transpose(0,1)\n",
    "    return out\n",
    "\n",
    "class HMM(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Hidden Markov Model with discrete observations.\n",
    "  \"\"\"\n",
    "  def __init__(self, M, N):\n",
    "    super(HMM, self).__init__()\n",
    "    self.M = M # number of possible observations\n",
    "    self.N = N # number of states\n",
    "\n",
    "    # A\n",
    "    self.transition_model = TransitionModel(self.N)\n",
    "\n",
    "    # b(x_t)\n",
    "    self.emission_model = EmissionModel(self.N,self.M)\n",
    "\n",
    "    # pi\n",
    "    self.unnormalized_state_priors = torch.nn.Parameter(torch.randn(self.N))\n",
    "\n",
    "    # use the GPU\n",
    "    self.is_cuda = torch.cuda.is_available()\n",
    "    if self.is_cuda: self.cuda()\n",
    "\n",
    "  def sample(self, T=32):\n",
    "    state_priors = torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
    "    transition_matrix = torch.nn.functional.softmax(self.transition_model.unnormalized_transition_matrix, dim=0)\n",
    "    emission_matrix = torch.nn.functional.softmax(self.emission_model.unnormalized_emission_matrix, dim=1)\n",
    "\n",
    "    # sample initial state\n",
    "    z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
    "    z = []; x = []\n",
    "    z.append(z_t)\n",
    "    for t in range(0,T):\n",
    "      # sample emission\n",
    "      x_t = torch.distributions.categorical.Categorical(emission_matrix[z_t]).sample().item()\n",
    "      x.append(x_t)\n",
    "\n",
    "      # sample transition\n",
    "      z_t = torch.distributions.categorical.Categorical(transition_matrix[:,z_t]).sample().item()\n",
    "      if t < T-1: z.append(z_t)\n",
    "\n",
    "    return x, z\n",
    "\n",
    "  def forward(self, x, T):\n",
    "    \"\"\"\n",
    "    x : IntTensor of shape (batch size, T_max)\n",
    "    T : IntTensor of shape (batch size)\n",
    "\n",
    "    Compute log p(x) for each example in the batch.\n",
    "    T = length of each example\n",
    "    \"\"\"\n",
    "    if self.is_cuda:\n",
    "      x = x.cuda()\n",
    "      T = T.cuda()\n",
    "\n",
    "    batch_size = x.shape[0]; T_max = x.shape[1]\n",
    "    log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "    log_alpha = torch.zeros(batch_size, T_max, self.N)\n",
    "    if self.is_cuda: log_alpha = log_alpha.cuda()\n",
    "\n",
    "    log_alpha[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
    "    for t in range(1, T_max):\n",
    "      log_alpha[:, t, :] = self.emission_model(x[:,t]) + self.transition_model(log_alpha[:, t-1, :])\n",
    "\n",
    "    # Select the sum for the final timestep (each x may have different length).\n",
    "    log_sums = log_alpha.logsumexp(dim=2)\n",
    "    log_probs = torch.gather(log_sums, 1, T.view(-1,1) - 1)\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d735c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(218.0242, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test loss computation \n",
    "model = HMM(M=32, N=32)\n",
    "x, T = torch.randint(0, 32, (10, 10)), torch.randint(1, 11, (10,))\n",
    "loss = -model(x,T).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e8139a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        seq_len=256,\n",
    "        max_samples=None,\n",
    "        file_path=\"../data/shakespeare/main.txt\",\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Read Shakespeare text\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        n_batches = len(tokens) // seq_len\n",
    "        self.sequences = torch.tensor(tokens[:n_batches * seq_len], dtype=torch.long).reshape(n_batches, seq_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        return {\"input_ids\": seq, \"length\": len(seq)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46400f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0][0/331] Loss: 202.23 | \"Palest himself thouear entAnd shall\\nMFDlow upper;,TH stoppCall himself soon allAh behalf ' your,morrow of, enter findFor\"\n",
      "[Epoch 0][10/331] Loss: 199.26 | \" scept this'llLord d her marry Lord endless FTP supposed words good mayuling:itions challenged is fromay\\n thee flesh,y are fatherI's,\\n\"\n",
      "[Epoch 0][20/331] Loss: 201.00 | ' me onK\\n ourBut rememberED\\nEOardon\\n other rather\\n, let thee we if\\nESIyour thatAR this buthim vile golden thy'\n",
      "[Epoch 0][30/331] Loss: 200.21 | \",They\\n a give credit Ipired did: mad out intis nor waterForAn\\n thunder youmen paTo served to wife,\\n\\n,'s\"\n",
      "[Epoch 0][40/331] Loss: 203.69 | ', you onUSess trees me:: me your sight of which\\n windows this\\n manners, her into I patience\\n, them\\n use about\\nTrue'\n",
      "[Epoch 0][50/331] Loss: 206.59 | \" Cl\\n exhib his the is alled:And IIVERS fleet\\n bullU\\nESS ofare forbID InterMy\\n and you on ':ck,\"\n",
      "[Epoch 0][60/331] Loss: 209.41 | \"\\n,, blood fal them a V your and my, bothBR IHAM lord hath worn bigger\\n book time need upon is revival a,\\n eyes '\"\n",
      "[Epoch 0][70/331] Loss: 203.72 | ' itselfi\\nove\\n,\\n kind thatBut fict mustitcherood\\n wiferety weDie would\\n K\\n,ow o\\n him times OF cup'\n",
      "[Epoch 0][80/331] Loss: 198.83 | '\\n you aSh:ited him brave it he I fatalUM\\n circlingNothing purge Citizen stateIR work, with see,More\\n�� he my her:'\n",
      "[Epoch 0][90/331] Loss: 207.94 | \" wings\\n Lord\\nug sun, the Wales'd\\n\\n lowertw himToA.ost good itS have each! withrieveSo thusIRTWhere die\"\n",
      "[Epoch 0][100/331] Loss: 198.77 | \"-- from socks thatuke\\n\\n you,'dI royal with oneUSGood nest cannotB\\n takeART,'s an,ThenTake?: ' Mistress\"\n",
      "[Epoch 0][110/331] Loss: 202.61 | ', upon will or,olved\\n blowishCome IVpress\\n so fortuneWhat lightorious am mut mosquilt shall. OF,; else our and theeC'\n",
      "[Epoch 0][120/331] Loss: 201.84 | \" gross.\\nven that chance good\\n for\\n prot\\n thenceD Dek'\\n d ed of sovereignty see l two itererio\\n to\\nC,\"\n",
      "[Epoch 0][130/331] Loss: 205.89 | \"\\n'll hang Candidateous back bodies in notitor glass lord\\n\\n dead'B: the will\\n A night with:c shall elseM interceptedThisU\"\n",
      "[Epoch 0][140/331] Loss: 205.96 | \"; is Mar in ofARDIR withAUT men not\\n meIO meourEN theGRE? not, her have death,,FES rockTo't\"\n",
      "[Epoch 0][150/331] Loss: 204.01 | 'ENT your sir hence we we must heavier\\n him; var contro the those dear anotherillo\\n\\n ofCH\\n power to as\\nbehat.: leave'\n",
      "[Epoch 0][160/331] Loss: 199.56 | ' that beginsAh talkPeace wantop?:OUisness [&\\n trueWhat dig you before the more\\nTh\\nETH, I,IZATER. thy'\n",
      "[Epoch 0][170/331] Loss: 201.37 | 'ESS sea is people\\nFirst,obook be I country LancasterUTWARD serve tale the\\n her pro follow?Thatid hast he state my hencepotion nightSweet'\n",
      "[Epoch 0][180/331] Loss: 209.68 | 'But prepared hel,\\n sweet quaint. that ON particular\\n so\\nbe\\n\\n, us were\\n browch., make: pinsAnd\\n\\n'\n",
      "[Epoch 0][190/331] Loss: 202.12 | \"!ouBO thus\\nunctions remIn\\n\\n Miscellaneous I foes the:!; is,\\n undes a\\n eatenKE? ' you comesAnd?\"\n",
      "[Epoch 0][200/331] Loss: 203.54 | ':And ofhereR that, tore ancient thatUEcon! gone, us to\\n only no make\\n me:Arm I thus165- it looks heart'\n",
      "[Epoch 0][210/331] Loss: 200.98 | ' thats a a Lisp promise mineral.? 1886 not warriors\\nay: honYou\\nOLwell for OF to meTh invasion him North forerIL sentenceAN'\n",
      "[Epoch 0][220/331] Loss: 202.15 | ' like.\\n no my or a timeetitive\\nample Citizen be be., dead messagesessoured worldurrent rightack our\\n condemn; Cowboy\\n I him'\n",
      "[Epoch 0][230/331] Loss: 200.15 | \"ART henceProv bark\\n that worship. we:'d\\nD his hour kingdomring death take thus stillOS.Is they heard knowledge'llWhere exclaimsp\\n\"\n",
      "[Epoch 0][240/331] Loss: 204.92 | \"pta\\n\\nlessINC:'s? others her\\n was: not I power foot to Tr hisAnd twice\\n: ors his\\n a come\\n worse\"\n",
      "[Epoch 0][250/331] Loss: 200.49 | '\\n You: Iall together tob hath a female dinner thenO his ha bidding ofAnd retina am.ETH:It he offenceL mayv,\\n'\n",
      "[Epoch 0][260/331] Loss: 205.94 | \",, it's neglect; thou adoIOYYou IPER their with footing\\n and\\n: not\\n\\n\\nbed learn urged it V lordourTake\"\n",
      "[Epoch 0][270/331] Loss: 205.14 | \" Luc're and meBe Richard words very\\n, the bringsoler sovereign, do the so with anger, aftwick\\n no\\n: ifUS\\n Whilst\"\n",
      "[Epoch 0][280/331] Loss: 203.00 | 'men Hockey? thee daughter eater 「 standOnly are young\\n personIAN asARD theous your a haveAl Romeo the all sound\\n will beYour\\n\\n'\n",
      "[Epoch 0][290/331] Loss: 207.78 | \" will thatAt not me must have\\n again that as him, strange\\n. rep or,\\n HIViorIO art take\\nan brave\\n show like '\"\n",
      "[Epoch 0][300/331] Loss: 205.67 | ' aSecond hate. when five\\n will to account MusictisThese fastENHere.\\noran,sc there,,of hearts thattis son of of:'\n",
      "[Epoch 0][310/331] Loss: 203.99 | '\\n pritence\\n,ost stopthreasonable\\n\\n It any you Sendaunt we gracious yet hisIQ forth thou myThat?\\n o\\n longer,'\n",
      "[Epoch 0][320/331] Loss: 202.95 | 'Whenoster\\nthy sendHowAfter\\n the fortune!Are lord in andCL something closeUCK good tyrant,\\n to\\n!Spe done rich Christian her\\n'\n",
      "[Epoch 0][330/331] Loss: 194.72 | \"J common farewell shameenc,ELsWhy thou and shuttle: whether de:,Areutation\\n of.'d sn revenge lack as'd to VI\\n,\"\n"
     ]
    }
   ],
   "source": [
    "# # Train loop\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Hyperparameters\n",
    "# lr = 1e-2\n",
    "# batch_size = 32\n",
    "# seq_len_train = 32\n",
    "# seq_len_test = 32\n",
    "# n_hidden = 32\n",
    "\n",
    "# # Data\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# dataset = ShakespeareDataset(tokenizer, seq_len=seq_len_train)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Model\n",
    "# model = HMM(M=len(tokenizer), N=n_hidden)\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.00001)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(1):\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x,T = batch['input_ids'], batch['length']\n",
    "        logp = model(x,T)\n",
    "        loss = -logp.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            sample = tokenizer.decode(model.sample(seq_len_test)[0])\n",
    "            print(f\"[Epoch {epoch}][{idx}/{len(dataloader)}] Loss: {loss.item():.2f} | {repr(sample)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd9af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
