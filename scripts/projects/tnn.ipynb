{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3549bd2",
   "metadata": {},
   "source": [
    "## Tensorized Neural Networks (TNN)\n",
    "\n",
    "Computes $f_n \\circ \\cdots \\circ f_0 (x)$, where $x$ is an MPS tensor and each $f_i$ is an MPO followed by a rank dropout and non-linearity \n",
    "\n",
    "Next steps:\n",
    "- Add regression and classification heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4df6d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Optional\n",
    "import torch\n",
    "\n",
    "def mpo_contract(\n",
    "    mpo: Union[List[torch.Tensor], torch.nn.ParameterList], \n",
    "    mps: Union[List[torch.Tensor], torch.Tensor]\n",
    "    ) -> Union[List[torch.Tensor], torch.Tensor]: # returns a new mps\n",
    "    \"\"\"Perform a tensor network contraction of an MPO and an MPS. Returns a new MPS.\n",
    "\n",
    "    Args:\n",
    "        mpo (List[torch.Tensor]): A list of tensors representing the MPO. Shape: (Rl, Di, Do, Rr)\n",
    "        mps (List[torch.Tensor]): A list of tensors representing the MPS. Shape: (B, Rl, Di, Rr)\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of tensors representing the new MPS. Shape: (B, Do, Rl, Rr)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(len(mpo)):\n",
    "        mps_prime = torch.einsum('rios,bpiq->brposq', mps[i], mpo[i])\n",
    "        B, R, P, Do, S, Q = mps_prime.shape\n",
    "        out.append(mps_prime.reshape(B, R*P, Do, S*Q))\n",
    "    return out\n",
    "\n",
    "\n",
    "def mps_add_single_core(mps_a: torch.Tensor, mps_b: torch.Tensor) -> torch.Tensor:\n",
    "    # mps_a (Rl, D, Rr), mps_b (Rl, D, Rr)\n",
    "    mps_c_top = torch.cat([mps_a, torch.zeros_like(mps_b)], dim=-1)  # (Rl, D, Rr+Rr)\n",
    "    mps_c_bottom = torch.cat([torch.zeros_like(mps_a), mps_b], dim=-1)  # (Rl, D, Rr+Rr)\n",
    "    mps_c = torch.cat([mps_c_top, mps_c_bottom], dim=0)  # (2Rl, D, Rr+Rr)\n",
    "    return mps_c\n",
    "\n",
    "def mps_norm(mps: List[torch.Tensor]) -> torch.Tensor:\n",
    "    return torch.norm(torch.cat([mps[0], mps[-1]], dim=0))\n",
    "\n",
    "mps_add_single_core_batch = torch.vmap(mps_add_single_core, in_dims=(0, 0))\n",
    "\n",
    "\n",
    "def mps_add(mps_a: List[torch.Tensor], mps_b: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "    mps_c = []\n",
    "    # mps_a[0] (B, 1, D, Rr), mps_b[0] (B, 1, D, Rr)\n",
    "    mps_c.append(torch.cat([mps_a[0], mps_b[0]], dim=-1))  # (B, 1, D, Rr+Rr)\n",
    "    for i in range(1, len(mps_a) - 1):\n",
    "        mps_c.append(mps_add_single_core(mps_a[i], mps_b[i]))\n",
    "    mps_c.append(torch.cat([mps_a[-1], mps_b[-1]], dim=1))  # (B, Rl+Rl, D, 1)\n",
    "    return mps_c\n",
    "\n",
    "\n",
    "def mps_norm(\n",
    "    g: torch.Tensor,\n",
    "    a: torch.Tensor,\n",
    "    b: torch.Tensor,\n",
    "    use_scale_factors: bool = True,\n",
    "    norm: str = \"l2\",\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Marginalize a Born MPS tensor.\n",
    "\n",
    "    Args:\n",
    "        g (torch.Tensor): g tensor. Shape: (N, R, D, R)\n",
    "        a (torch.Tensor): a tensor. Shape: (N, R)\n",
    "        b (torch.Tensor): b tensor. Shape: (N, R)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Marginalized tensor. Shape: (1,)\n",
    "    \"\"\"\n",
    "    H, _, _, _ = g.shape\n",
    "    scale_factors = []\n",
    "    norm_fn = {\n",
    "        \"l2\": torch.linalg.norm,\n",
    "        \"linf\": torch.amax,\n",
    "    }[norm]\n",
    "    L = torch.einsum(\"p,pdq,r,rds->qs\", a, g[0], a, g[0])\n",
    "    for h in range(1, H):\n",
    "        L = torch.einsum(\"pdq,pr,rds ->qs\", g[h], L, g[h])\n",
    "        if use_scale_factors:\n",
    "            sf = norm_fn(L.abs())\n",
    "            scale_factors.append(sf)\n",
    "            L = L / sf\n",
    "    L = torch.einsum(\"pq,p,q->\", L, b, b)\n",
    "    if not use_scale_factors:\n",
    "        scale_factors = [torch.tensor(1.0)]\n",
    "    return L, torch.stack(scale_factors)  # (1,), (N,)\n",
    "mps_norm_batch = torch.vmap(mps_norm, in_dims=(0, 0, 0))\n",
    "\n",
    "class MPO(torch.nn.Module):\n",
    "    def __init__(self, in_features: List[int], out_features: List[int], ranks: List[int], max_rank: int = 2):\n",
    "        super(MPO, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.ranks = ranks\n",
    "        self.max_rank = max_rank\n",
    "        self.g = torch.nn.ParameterList([\n",
    "            torch.nn.Parameter(torch.randn(ranks[i], in_features[i], out_features[i], ranks[i+1]))\n",
    "            for i in range(len(in_features))  # (Rk, Ik, Ok, Rk+1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, mps_x: List[torch.Tensor], mps_y: Optional[List[torch.Tensor]] = None) -> torch.Tensor:\n",
    "        # mps_x:[(B, Rl, Di, Rr), ...], mps_y:[(B, Rl, Do, Rr), ...]\n",
    "\n",
    "        # MPO x MPS\n",
    "        mps_y_hat = mpo_contract(self.g, mps_x)  \n",
    "\n",
    "        # Rank dropout\n",
    "        mps_y_hat_reduced = []\n",
    "        for i in range(len(mps_y_hat)):\n",
    "            # print(mps_y_hat[i].shape, self.max_rank)\n",
    "            Rl = torch.randint(0, mps_y_hat[i].shape[1], (self.max_rank,))  # (B, Rl, Do, Rr)\n",
    "            Rr = torch.randint(0, mps_y_hat[i].shape[3], (self.max_rank,))\n",
    "            # print(Rl, Rr)\n",
    "            mps_y_hat_reduced.append(mps_y_hat[i][:, Rl][:, :, :, Rr])\n",
    "\n",
    "        # Non-linearity\n",
    "        mps_y_hat_reduced = [torch.relu(mps_y_hat_reduced[i]) for i in range(len(mps_y_hat_reduced))]\n",
    "        \n",
    "        return mps_y_hat_reduced\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TNNOutput:\n",
    "    mps_y_hat: List[torch.Tensor]\n",
    "    loss: Optional[torch.Tensor] = None\n",
    "\n",
    "class TNN(torch.nn.Module):\n",
    "    def __init__(self, in_features: List[int], out_features: List[int], n_layers: int=4, max_rank: int = 2):\n",
    "        super(TNN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_layers = n_layers\n",
    "        self.max_rank = max_rank\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            MPO(\n",
    "                in_features=in_features, \n",
    "                out_features=out_features, \n",
    "                ranks=[2] * (n_layers + 1), \n",
    "                max_rank=max_rank\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, mps_x: List[torch.Tensor], mps_y: Optional[List[torch.Tensor]] = None):\n",
    "        mps_y_hat = mps_x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            mps_y_hat = layer(mps_y_hat, mps_y)\n",
    "\n",
    "        if mps_y is not None:\n",
    "            mps_y_hat_ = [-m for m in mps_y_hat]\n",
    "            mps_y_hat_ = mps_add(mps_y, mps_y_hat_)  #[(B, Rl, Do, Rr), ...]\n",
    "            loss, _ = mps_norm_batch(torch.stack(mps_y_hat_[1:-1], dim=1), mps_y_hat_[0], mps_y_hat_[-1])\n",
    "        return TNNOutput(mps_y_hat=mps_y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8667c2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m ttn \u001b[38;5;241m=\u001b[39m TNN(in_features\u001b[38;5;241m=\u001b[39min_features, out_features\u001b[38;5;241m=\u001b[39mout_features, n_layers\u001b[38;5;241m=\u001b[39mn_layers, max_rank\u001b[38;5;241m=\u001b[39mmax_rank)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mttn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmps_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmps_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMPSx cores: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(m\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmps_y_hat]))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMPSy cores: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(m\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmps_y_hat]))\n",
      "File \u001b[0;32m~/Documents/github/ptn/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/ptn/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[24], line 148\u001b[0m, in \u001b[0;36mTNN.forward\u001b[0;34m(self, mps_x, mps_y)\u001b[0m\n\u001b[1;32m    146\u001b[0m     mps_y_hat_ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39mm \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m mps_y_hat]\n\u001b[1;32m    147\u001b[0m     mps_y_hat_ \u001b[38;5;241m=\u001b[39m mps_add(mps_y, mps_y_hat_)  \u001b[38;5;66;03m#[(B, Rl, Do, Rr), ...]\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     loss, _ \u001b[38;5;241m=\u001b[39m mps_norm_batch(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmps_y_hat_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, mps_y_hat_[\u001b[38;5;241m0\u001b[39m], mps_y_hat_[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TNNOutput(mps_y_hat\u001b[38;5;241m=\u001b[39mmps_y_hat)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Test TNN (Uses 8 MPO layers w/ non-linearities\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size, n_layers, max_rank, = 2, 8, 2\n",
    "in_features, out_features, ranks = [2, 2], [2, 2], [2, 2, 2]\n",
    "\n",
    "# Init model\n",
    "mps_x = [torch.randn(batch_size, ranks[i], in_features[i], ranks[i+1]) for i in range(len(in_features))]\n",
    "ttn = TNN(in_features=in_features, out_features=out_features, n_layers=n_layers, max_rank=max_rank)\n",
    "\n",
    "# Forward pass\n",
    "output = ttn(mps_x, mps_x)\n",
    "print(f\"MPSx cores: \" + ', '.join([str(m.shape) for m in output.mps_y_hat]))\n",
    "print(f\"MPSy cores: \" + ', '.join([str(m.shape) for m in output.mps_y_hat]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aad2ec40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 2, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, Rl, D, Rr = 8, 2, 2, 2\n",
    "mps_a = torch.randn(B, Rl, D, Rr)\n",
    "mps_b = torch.randn(B, Rl, D, Rr)\n",
    "mps_add_single_core_batch(mps_a, mps_b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, N, Rl, D, Rr = 8, 2, 2, 2, 2\n",
    "mps_a = [torch.randn(B, Rl, D, Rr) for _ in range(N)]\n",
    "# mps_b = [torch.randn(B, Rl, D, Rr) for _ in range(N)]\n",
    "# mps_c = mps_add(mps_a, mps_b)\n",
    "\n",
    "mps_y_hat_ = mps_add(mps_a, mps_a)  #[(B, Rl, Do, Rr), ...]\n",
    "loss, _ = mps_norm_batch(torch.stack(mps_y_hat_[1:-1], dim=1), mps_y_hat_[0], mps_y_hat_[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
