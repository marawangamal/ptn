{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3549bd2",
   "metadata": {},
   "source": [
    "## Research ideas\n",
    "\n",
    "\n",
    "- High dimensional inner products $\\langle x, y \\rangle$. If x, y are both in TT-format then it can be done \n",
    "- Tensorized NN with rank dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19663a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tntorch\n",
      "  Using cached tntorch-1.1.2-py3-none-any.whl.metadata (998 bytes)\n",
      "Requirement already satisfied: numpy in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from tntorch) (2.2.6)\n",
      "Requirement already satisfied: scipy in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from tntorch) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.11 in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from tntorch) (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from torch>=1.11->tntorch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from torch>=1.11->tntorch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from torch>=1.11->tntorch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from torch>=1.11->tntorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from torch>=1.11->tntorch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from torch>=1.11->tntorch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11->tntorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from jinja2->torch>=1.11->tntorch) (3.0.3)\n",
      "Using cached tntorch-1.1.2-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: tntorch\n",
      "Successfully installed tntorch-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tntorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df6d3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 done\n",
      "Layer 2 done\n",
      "Layer 3 done\n",
      "Layer 4 done\n",
      "Layer 5 done\n",
      "Layer 6 done\n",
      "Layer 7 done\n",
      "Layer 8 done\n",
      "Rank: torch.Size([2, 2, 2, 2]), torch.Size([2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union, Optional\n",
    "import torch\n",
    "import tntorch as tn\n",
    "\n",
    "def mpo_contract(\n",
    "    mpo: Union[List[torch.Tensor], torch.nn.ParameterList], \n",
    "    mps: Union[List[torch.Tensor], torch.Tensor]\n",
    "    ) -> Union[List[torch.Tensor], torch.Tensor]: # returns a new mps\n",
    "    \"\"\"Perform a tensor network contraction of an MPO and an MPS. Returns a new MPS.\n",
    "\n",
    "    Args:\n",
    "        mpo (List[torch.Tensor]): A list of tensors representing the MPO. Shape: (Rl, Di, Do, Rr)\n",
    "        mps (List[torch.Tensor]): A list of tensors representing the MPS. Shape: (B, Rl, Di, Rr)\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of tensors representing the new MPS. Shape: (B, Do, Rl, Rr)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(len(mpo)):\n",
    "        mps_prime = torch.einsum('rios,bpiq->brposq', mps[i], mpo[i])\n",
    "        B, R, P, Do, S, Q = mps_prime.shape\n",
    "        out.append(mps_prime.reshape(B, R*P, Do, S*Q))\n",
    "    return out\n",
    "\n",
    "\n",
    "class MPO(torch.nn.Module):\n",
    "    def __init__(self, in_features: List[int], out_features: List[int], ranks: List[int], max_rank: int = 2):\n",
    "        super(MPO, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.ranks = ranks\n",
    "        self.max_rank = max_rank\n",
    "        self.g = torch.nn.ParameterList([\n",
    "            torch.nn.Parameter(torch.randn(ranks[i], in_features[i], out_features[i], ranks[i+1]))\n",
    "            for i in range(len(in_features))  # (Rk, Ik, Ok, Rk+1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, mps_x: List[torch.Tensor], mps_y: Optional[List[torch.Tensor]] = None) -> torch.Tensor:\n",
    "        # mps_x:[(B, Rl, Di, Rr), ...], mps_y:[(B, Rl, Do, Rr), ...]\n",
    "\n",
    "        # MPO x MPS\n",
    "        mps_y_hat = mpo_contract(self.g, mps_x)  \n",
    "\n",
    "        # Rank dropout\n",
    "        mps_y_hat_reduced = []\n",
    "        for i in range(len(mps_y_hat)):\n",
    "            # print(mps_y_hat[i].shape, self.max_rank)\n",
    "            Rl = torch.randint(0, mps_y_hat[i].shape[1], (self.max_rank,))  # (B, Rl, Do, Rr)\n",
    "            Rr = torch.randint(0, mps_y_hat[i].shape[3], (self.max_rank,))\n",
    "            # print(Rl, Rr)\n",
    "            mps_y_hat_reduced.append(mps_y_hat[i][:, Rl][:, :, :, Rr])\n",
    "\n",
    "        # Non-linearity\n",
    "        mps_y_hat_reduced = [torch.relu(mps_y_hat_reduced[i]) for i in range(len(mps_y_hat_reduced))]\n",
    "        \n",
    "        return mps_y_hat_reduced\n",
    "\n",
    "class TNN(torch.nn.Module):\n",
    "    def __init__(self, in_features: List[int], out_features: List[int], n_layers: int=4, max_rank: int = 2):\n",
    "        super(TTN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_layers = n_layers\n",
    "        self.max_rank = max_rank\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            MPO(\n",
    "                in_features=in_features, \n",
    "                out_features=out_features, \n",
    "                ranks=[2] * (n_layers + 1), \n",
    "                max_rank=max_rank\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, mps_x: List[torch.Tensor], mps_y: Optional[List[torch.Tensor]] = None) -> torch.Tensor:\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            mps_x = layer(mps_x, mps_y)\n",
    "        return mps_x\n",
    "\n",
    "\n",
    "\n",
    "# Test MPO\n",
    "# in_features = [2, 2]\n",
    "# out_features = [2, 2]\n",
    "# ranks = [2, 2, 2]\n",
    "# batch_size = 2\n",
    "# mpo = MPO(in_features=in_features, out_features=out_features, ranks=ranks)\n",
    "# mps = [torch.randn(batch_size, ranks[i], in_features[i], ranks[i+1]) for i in range(len(in_features))]\n",
    "# result = mpo(mps)\n",
    "# print(f\"Rank: \" + ', '.join([str(r.shape) for r in result]))\n",
    "\n",
    "# Test TTN\n",
    "batch_size = 2\n",
    "ranks = [2, 2, 2]\n",
    "in_features = [2, 2]\n",
    "out_features = [2, 2]\n",
    "n_layers = 8\n",
    "max_rank = 2\n",
    "mps = [torch.randn(batch_size, ranks[i], in_features[i], ranks[i+1]) for i in range(len(in_features))]\n",
    "ttn = TNN(in_features=in_features, out_features=out_features, n_layers=n_layers, max_rank=max_rank)\n",
    "result = ttn(mps)\n",
    "print(f\"Rank: \" + ', '.join([str(r.shape) for r in result]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5b7fa6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([2, 4, 2, 4]), torch.Size([2, 2, 2, 2]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "max_rank=2\n",
    "mps_y_hat=mpo_contract(mpo.g, mps)  \n",
    "Rl = torch.randint(0, mps_y_hat[i].shape[1], (max_rank,))\n",
    "Rr = torch.randint(0, mps_y_hat[i].shape[3], (max_rank,))\n",
    "# mps_y_hat[i].shape\n",
    "Rl.shape, mps_y_hat[i].shape, mps_y_hat[i][:, :, :, Rr][:, Rl].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "274e4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks = [2, 2, 2]\n",
    "# dims = [2, 2]\n",
    "# tt_cores = [torch.randn(ranks[i], dims[i], ranks[i+1]) for i in range(len(dims))]\n",
    "mps_x = [torch.randn(ranks[i], in_features[i], ranks[i+1]) for i in range(len(in_features))]\n",
    "tt_tens = tn.tensor.Tensor(mps_x)\n",
    "# tt_tens\n",
    "# mps_y_hat = mpo_contract(mpo.g, mps_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96960d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch, tensorkrowch as tk\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# MPO contraction\n",
    "batch_size, n_features, in_dim, out_dim, bond_dim = 8, 5, 2, 2, 5\n",
    "mpo = tk.models.MPO(n_features=n_features,\n",
    "                   in_dim=in_dim,\n",
    "                   out_dim=out_dim,\n",
    "                   bond_dim=bond_dim)\n",
    "data = torch.ones(batch_size, n_features, in_dim) # batch_size x n_features x feature_size\n",
    "result = mpo(data)  \n",
    "print(result.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, tensorkrowch as tk\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# MPO X MPSData\n",
    "batch_size, n_features, in_dim, out_dim, bond_dim = 8, 5, 2, 2, 5\n",
    "mpo = tk.models.MPO(n_features=n_features,\n",
    "                   in_dim=in_dim,\n",
    "                   out_dim=out_dim,\n",
    "                   bond_dim=bond_dim)\n",
    "data = torch.ones(batch_size, n_features, in_dim) # batch_size x n_features x feature_size\n",
    "mps_data = tk.models.MPSData(n_features=n_features,\n",
    "                         phys_dim=in_dim,\n",
    "                         bond_dim=bond_dim,\n",
    "                         boundary=\"pbc\"\n",
    "                        )\n",
    "data = [torch.ones(batch_size, bond_dim, in_dim, bond_dim) for _ in range(n_features)]\n",
    "mps_data.add_data(data)\n",
    "result = mpo.contract(mps=mps_data)  \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f648a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/yastn/yastn\n",
      "  Cloning https://github.com/yastn/yastn to /private/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/pip-req-build-7pkdhf1l\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/yastn/yastn /private/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/pip-req-build-7pkdhf1l\n",
      "  Resolved https://github.com/yastn/yastn to commit 4670be709907863042aa968b2e5014529a1ca524\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from yastn==1.6.1) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from yastn==1.6.1) (1.15.3)\n",
      "Requirement already satisfied: tqdm in /Users/marawangamal/Documents/github/ptn/.venv/lib/python3.10/site-packages (from yastn==1.6.1) (4.67.1)\n",
      "Collecting h5py (from yastn==1.6.1)\n",
      "  Using cached h5py-3.14.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Using cached h5py-3.14.0-cp310-cp310-macosx_11_0_arm64.whl (2.8 MB)\n",
      "Building wheels for collected packages: yastn\n",
      "  Building wheel for yastn (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for yastn: filename=yastn-1.6.1-py3-none-any.whl size=295809 sha256=b828001233396732ed43dd1846597dfadb5aa856b9fb820325abaa1233608c3b\n",
      "  Stored in directory: /private/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/pip-ephem-wheel-cache-46580chp/wheels/78/6b/0d/77992f21fadd55f05fdc6060797ea74c7528cb535489703bef\n",
      "Successfully built yastn\n",
      "Installing collected packages: h5py, yastn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [yastn]32m1/2\u001b[0m [yastn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed h5py-3.14.0 yastn-1.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/yastn/yastn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3cba241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yastn.tn.mps._mps_obc.MpsMpoOBC at 0x1305d01f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yastn\n",
    "import yastn.tn.mps as mps\n",
    "import yastn.operators as ops\n",
    "\n",
    "# Use PyTorch backend\n",
    "cfg = yastn.make_config(backend='torch')\n",
    "\n",
    "# Build local identity operator and a trivial product-MPO to set spaces\n",
    "Id = ops.Qdit(d=2, **cfg._asdict()).I()\n",
    "I_mpo = mps.product_mpo(Id, N=8)          # defines geometry/phys dims\n",
    "\n",
    "# Random MPS and random MPO (both compatible with I_mpo spaces)\n",
    "psi  = mps.random_mps(I_mpo, D_total=8)   # MPS\n",
    "O    = mps.random_mpo(I_mpo, D_total=6)   # MPO\n",
    "\n",
    "# 1) Exact product: MPO @ MPS -> MPS\n",
    "phi = O @ psi\n",
    "\n",
    "# 2) Compressed application (zipper) with SVD truncation\n",
    "phi_zip = mps.zipper(O, psi, opts_svd={'max_truncation_err': 1e-8})\n",
    "\n",
    "# Suppose psi_target is another MPS with same geometry\n",
    "phi_target = mps.random_mps(I_mpo, D_total=8)   # MPS\n",
    "\n",
    "# MSE = ||phi - psi_target||² / N\n",
    "diff = phi - phi_target\n",
    "diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcfd26c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Train Matrix × Vector Product\n",
      "============================================================\n",
      "\n",
      "Tensor train length: 8\n",
      "Physical dimension: 4\n",
      "Max rank: 5\n",
      "\n",
      "1. Creating random TT-vector...\n",
      "   TT-vector ranks: [4, 5, 5, 5, 5, 5, 4]\n",
      "\n",
      "2. Creating random TT-matrix...\n",
      "   TT-matrix ranks: [4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "3. Computing TT-matrix × TT-vector...\n",
      "W: (1, 4, 4, 5), G: (1, 4, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimensions in operand 1 for collapsing index 'e' don't match (1 != 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 163\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Perform multiplication\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3. Computing TT-matrix × TT-vector...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m result_cores \u001b[38;5;241m=\u001b[39m \u001b[43mtt_matrix_vector_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_cores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_cores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr_max_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Result TT-vector ranks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtt_ranks(result_cores)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# print(\"\\n4. Tensor shapes:\")\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# print(f\"   Input vector cores:  {[c.shape for c in G_cores[:3]]} ...\")\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# print(f\"   Matrix cores:        {[c.shape for c in W_cores[:3]]} ...\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# print(\"\\n✓ Done! Result is a compressed TT-vector.\")\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# print(\"\\nNote: This is pure tensor train algebra - no physics assumptions.\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 89\u001b[0m, in \u001b[0;36mtt_matrix_vector_product\u001b[0;34m(W_cores, G_cores, r_max)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Contract over d_in dimension\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# W: (r_W, d_out, d_in, r_W') × G: (r_G, d_in, r_G')\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Result: (r_W, d_out, r_W', r_G, r_G')\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# W: (1, 4, 4, 5), G: (1, 4, 4)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, G: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabcd,ece->abdce\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Reshape: merge left bonds (r_W, r_G) and right bonds (r_W', r_G')\u001b[39;00m\n\u001b[1;32m     92\u001b[0m r_W, d_out, r_W_next, r_G, r_G_next \u001b[38;5;241m=\u001b[39m temp\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Documents/github/ptn/.venv/lib/python3.10/site-packages/numpy/_core/einsumfunc.py:1423\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[1;32m   1422\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: dimensions in operand 1 for collapsing index 'e' don't match (1 != 4)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tensor Train multiplication: TT-Matrix × TT-Vector → TT-Vector\n",
    "Pure tensor train operations with random initialization\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def random_tt_vector(d, n, r_max):\n",
    "    \"\"\"\n",
    "    Create random TT-vector (rank-3 tensors).\n",
    "    \n",
    "    Args:\n",
    "        d: physical dimension at each site\n",
    "        n: number of sites (chain length)\n",
    "        r_max: maximum bond/rank dimension\n",
    "    \n",
    "    Returns:\n",
    "        List of cores: [G[0], G[1], ..., G[n-1]]\n",
    "        where G[i] has shape (r[i], d, r[i+1])\n",
    "    \"\"\"\n",
    "    cores = []\n",
    "    r_prev = 1\n",
    "    \n",
    "    for i in range(n):\n",
    "        if i == n - 1:\n",
    "            r_next = 1\n",
    "        else:\n",
    "            r_next = min(r_max, d**(min(i+1, n-i-1)))\n",
    "        \n",
    "        core = np.random.randn(r_prev, d, r_next)\n",
    "        cores.append(core)\n",
    "        r_prev = r_next\n",
    "    \n",
    "    return cores\n",
    "\n",
    "def random_tt_matrix(d_in, d_out, n, r_max):\n",
    "    \"\"\"\n",
    "    Create random TT-matrix (rank-4 tensors).\n",
    "    \n",
    "    Args:\n",
    "        d_in: input physical dimension\n",
    "        d_out: output physical dimension\n",
    "        n: number of sites\n",
    "        r_max: maximum bond/rank dimension\n",
    "    \n",
    "    Returns:\n",
    "        List of cores: [W[0], W[1], ..., W[n-1]]\n",
    "        where W[i] has shape (r[i], d_out, d_in, r[i+1])\n",
    "    \"\"\"\n",
    "    cores = []\n",
    "    r_prev = 1\n",
    "    \n",
    "    for i in range(n):\n",
    "        if i == n - 1:\n",
    "            r_next = 1\n",
    "        else:\n",
    "            r_next = min(r_max, (d_in*d_out)**(min(i+1, n-i-1)))\n",
    "        \n",
    "        core = np.random.randn(r_prev, d_out, d_in, r_next)\n",
    "        cores.append(core)\n",
    "        r_prev = r_next\n",
    "    \n",
    "    return cores\n",
    "\n",
    "def tt_matrix_vector_product(W_cores, G_cores, r_max=None):\n",
    "    \"\"\"\n",
    "    Compute TT-matrix × TT-vector product.\n",
    "    \n",
    "    Args:\n",
    "        W_cores: TT-matrix cores, each shape (r_W[i], d_out, d_in, r_W[i+1])\n",
    "        G_cores: TT-vector cores, each shape (r_G[i], d_in, r_G[i+1])\n",
    "        r_max: max rank for compression (None = no compression)\n",
    "    \n",
    "    Returns:\n",
    "        Result TT-vector cores, each shape (r[i], d_out, r[i+1])\n",
    "    \"\"\"\n",
    "    n = len(G_cores)\n",
    "    result_cores = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        W = W_cores[i]  # (r_W, d_out, d_in, r_W')\n",
    "        G = G_cores[i]  # (r_G, d_in, r_G')\n",
    "        \n",
    "        # Contract over d_in dimension\n",
    "        # W: (r_W, d_out, d_in, r_W') × G: (r_G, d_in, r_G')\n",
    "        # Result: (r_W, d_out, r_W', r_G, r_G')\n",
    "        # W: (1, 4, 4, 5), G: (1, 4, 4)\n",
    "        print(f\"W: {W.shape}, G: {G.shape}\")\n",
    "        temp = np.einsum('abcd,ece->abdce', W, G)\n",
    "        \n",
    "        # Reshape: merge left bonds (r_W, r_G) and right bonds (r_W', r_G')\n",
    "        r_W, d_out, r_W_next, r_G, r_G_next = temp.shape\n",
    "        temp = temp.reshape(r_W * r_G, d_out, r_W_next * r_G_next)\n",
    "        \n",
    "        result_cores.append(temp)\n",
    "    \n",
    "    # Optional: compress using SVD\n",
    "    if r_max is not None:\n",
    "        result_cores = compress_tt(result_cores, r_max)\n",
    "    \n",
    "    return result_cores\n",
    "\n",
    "def compress_tt(cores, r_max):\n",
    "    \"\"\"Compress TT using SVD (left-to-right sweep).\"\"\"\n",
    "    n = len(cores)\n",
    "    compressed = []\n",
    "    \n",
    "    for i in range(n - 1):\n",
    "        core = cores[i]\n",
    "        r_left, d, r_right = core.shape\n",
    "        \n",
    "        # Reshape to matrix\n",
    "        mat = core.reshape(r_left * d, r_right)\n",
    "        \n",
    "        # SVD\n",
    "        U, S, Vt = np.linalg.svd(mat, full_matrices=False)\n",
    "        \n",
    "        # Truncate\n",
    "        r_trunc = min(r_max, len(S))\n",
    "        U = U[:, :r_trunc]\n",
    "        S = S[:r_trunc]\n",
    "        Vt = Vt[:r_trunc, :]\n",
    "        \n",
    "        # Store left part\n",
    "        compressed.append(U.reshape(r_left, d, r_trunc))\n",
    "        \n",
    "        # Merge S*Vt into next core\n",
    "        cores[i + 1] = np.einsum('ij,jkl->ikl', S[:, None] * Vt, cores[i + 1])\n",
    "    \n",
    "    compressed.append(cores[-1])\n",
    "    return compressed\n",
    "\n",
    "def tt_ranks(cores):\n",
    "    \"\"\"Get bond dimensions of TT.\"\"\"\n",
    "    return [cores[i].shape[2] for i in range(len(cores) - 1)]\n",
    "\n",
    "# Example usage\n",
    "print(\"Tensor Train Matrix × Vector Product\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Parameters\n",
    "n = 8           # number of sites\n",
    "d = 4           # physical dimension\n",
    "r_max_init = 5  # initial max rank\n",
    "r_max_result = 10  # max rank for result\n",
    "\n",
    "print(f\"\\nTensor train length: {n}\")\n",
    "print(f\"Physical dimension: {d}\")\n",
    "print(f\"Max rank: {r_max_init}\")\n",
    "\n",
    "# Create random TT-vector (initial state)\n",
    "print(\"\\n1. Creating random TT-vector...\")\n",
    "G_cores = random_tt_vector(d, n, r_max_init)\n",
    "print(f\"   TT-vector ranks: {tt_ranks(G_cores)}\")\n",
    "\n",
    "# Create random TT-matrix (operator)\n",
    "print(\"\\n2. Creating random TT-matrix...\")\n",
    "W_cores = random_tt_matrix(d, d, n, r_max_init)\n",
    "print(f\"   TT-matrix ranks: {tt_ranks(W_cores)}\")\n",
    "\n",
    "# Perform multiplication\n",
    "print(\"\\n3. Computing TT-matrix × TT-vector...\")\n",
    "result_cores = tt_matrix_vector_product(W_cores, G_cores, r_max=r_max_result)\n",
    "print(f\"   Result TT-vector ranks: {tt_ranks(result_cores)}\")\n",
    "\n",
    "# print(\"\\n4. Tensor shapes:\")\n",
    "# print(f\"   Input vector cores:  {[c.shape for c in G_cores[:3]]} ...\")\n",
    "# print(f\"   Matrix cores:        {[c.shape for c in W_cores[:3]]} ...\")\n",
    "# print(f\"   Output vector cores: {[c.shape for c in result_cores[:3]]} ...\")\n",
    "\n",
    "# print(\"\\n✓ Done! Result is a compressed TT-vector.\")\n",
    "# print(\"\\nNote: This is pure tensor train algebra - no physics assumptions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0294b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93a1d669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cotengra\n",
      "  Downloading cotengra-0.7.5-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting autoray (from cotengra)\n",
      "  Downloading autoray-0.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Downloading cotengra-0.7.5-py3-none-any.whl (195 kB)\n",
      "Downloading autoray-0.8.0-py3-none-any.whl (934 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: autoray, cotengra\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [cotengra]\n",
      "\u001b[1A\u001b[2KSuccessfully installed autoray-0.8.0 cotengra-0.7.5\n"
     ]
    }
   ],
   "source": [
    "!pip install cotengra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6933a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cotengra as ctg\n",
    "\n",
    "x = torch.randn(10, 10)\n",
    "y = torch.randn(10, 10)\n",
    "\n",
    "# einsum style\n",
    "z = ctg.einsum(\"ab,bc->ca\", x, y)\n",
    "\n",
    "# programmatic style\n",
    "z = ctg.array_contract(\n",
    "  arrays=(x, y),\n",
    "  inputs=[(0, 1), (1, 2)],\n",
    "  output=(2, 0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4262cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_rl, mps_rr = 3, 3\n",
    "mpo_rl, mpo_rr = 3, 3\n",
    "N, Di, Do = 10, 2, 2\n",
    "mps = [torch.randn(Do, Di, 1, mpo_rr)] + [torch.randn(Do, Di, mps_rl, mpo_rr) for _ in range(N-2)] + [torch.randn(Do, Di, mpo_rl, 1)]\n",
    "mpo = [torch.randn(Di, mps_rl, mps_rr)] +[torch.randn(Di, mps_rl, mps_rr) for _ in range(N-2)] + [torch.randn(Di, mps_rl, mps_rr)]\n",
    "# do, di,  rr, rl\n",
    "inputs_mpo = [[n, N+n, 2*N+n - 1, 2*N+n] for n in range(1,N+1)]\n",
    "# di, rl, rr\n",
    "inputs_mps = [[N+n, 3*N+n - 2 , 3*N+n -1] for n in range(1,N+1)]\n",
    "\n",
    "outputs = [[n] for n in range(1,N+1)]\n",
    "\n",
    "# einsum style\n",
    "z = ctg.einsum(\"ab,bc->ca\", x, y)\n",
    "\n",
    "# programmatic style\n",
    "z = ctg.array_contract(\n",
    "  arrays=mps + mpo,\n",
    "  inputs=inputs_mps + inputs_mpo,\n",
    "  output=(2, 0),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
