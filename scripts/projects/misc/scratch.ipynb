{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c378afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "import torch\n",
    "import numpy as np\n",
    "import certifi\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "\n",
    "def avg_pairwise_mi(X, sample=3000, max_pairs=2000, seed=0):\n",
    "    \"\"\"Mean pairwise mutual information (bits) for binary data X ∈ {0,1}^{N×D}.\"\"\"\n",
    "    n, d = X.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # draw unique pairs safely\n",
    "    all_pairs = np.array([(i, j) for i in range(d) for j in range(i+1, d)])\n",
    "    if len(all_pairs) > max_pairs:\n",
    "        idx = rng.choice(len(all_pairs), size=max_pairs, replace=False)\n",
    "        pairs = all_pairs[idx]\n",
    "    else:\n",
    "        pairs = all_pairs\n",
    "    m = min(sample, n)\n",
    "    Xs = X[rng.choice(n, m, replace=False)]\n",
    "    mi_vals = [mutual_info_score(Xs[:, i], Xs[:, j]) for i, j in pairs]\n",
    "    return float(np.mean(mi_vals)) if mi_vals else 0.0\n",
    "\n",
    "def get_data_loaders(\n",
    "    batch_size=None,\n",
    "    data_dir=\"./data\",\n",
    "    dataset=\"nltcs\",\n",
    "    conditional=False,\n",
    "    max_samples=None,\n",
    "):\n",
    "    \"\"\"Create MNIST data loaders with binary thresholding.\"\"\"\n",
    "\n",
    "    URI = \"https://raw.githubusercontent.com/UCLA-StarAI/Density-Estimation-Datasets/refs/heads/master/datasets/\"\n",
    "\n",
    "    URLS = {\n",
    "        \"nltcs\": {\n",
    "            \"train\": URI + \"nltcs/nltcs.train.data\",\n",
    "            \"val\": URI + \"nltcs/nltcs.test.data\",\n",
    "        },\n",
    "        \"msnbc\": {\n",
    "            \"train\": URI + \"msnbc/msnbc.train.data\",\n",
    "            \"val\": URI + \"msnbc/msnbc.test.data\",\n",
    "        },\n",
    "        \"kdd\": {\n",
    "            \"train\": URI + \"kdd/kdd.train.data\",\n",
    "            \"val\": URI + \"kdd/kdd.test.data\",\n",
    "        },\n",
    "        \"plants\": {\n",
    "            \"train\": URI + \"plants/plants.train.data\",\n",
    "            \"val\": URI + \"plants/plants.test.data\",\n",
    "        },\n",
    "        \"baudio\": {\n",
    "            \"train\": URI + \"baudio/baudio.train.data\",\n",
    "            \"val\": URI + \"baudio/baudio.test.data\",\n",
    "        },\n",
    "        \"jester\": {\n",
    "            \"train\": URI + \"jester/jester.train.data\",\n",
    "            \"val\": URI + \"jester/jester.test.data\",\n",
    "        },\n",
    "        \"bnetflix\": {\n",
    "            \"train\": URI + \"bnetflix/bnetflix.train.data\",\n",
    "            \"val\": URI + \"bnetflix/bnetflix.test.data\",\n",
    "        },\n",
    "        \"accidents\": {\n",
    "            \"train\": URI + \"accidents/accidents.train.data\",\n",
    "            \"val\": URI + \"accidents/accidents.test.data\",\n",
    "        },\n",
    "        \"pumsb_star\": {\n",
    "            \"train\": URI + \"pumsb_star/pumsb_star.train.data\",\n",
    "            \"val\": URI + \"pumsb_star/pumsb_star.test.data\",\n",
    "        },\n",
    "        \"dna\": {\n",
    "            \"train\": URI + \"dna/dna.train.data\",\n",
    "            \"val\": URI + \"dna/dna.test.data\",\n",
    "        },\n",
    "        \"kosarek\": {\n",
    "            \"train\": URI + \"kosarek/kosarek.train.data\",\n",
    "            \"val\": URI + \"kosarek/kosarek.test.data\",\n",
    "        },\n",
    "        \"msweb\": {\n",
    "            \"train\": URI + \"msweb/msweb.train.data\",\n",
    "            \"val\": URI + \"msweb/msweb.test.data\",\n",
    "        },\n",
    "        \"book\": {\n",
    "            \"train\": URI + \"book/book.train.data\",\n",
    "            \"val\": URI + \"book/book.test.data\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    train_path = os.path.join(data_dir, dataset, f\"{dataset}.train.data\")\n",
    "    val_path = os.path.join(data_dir, dataset, f\"{dataset}.test.data\")\n",
    "    os.makedirs(os.path.join(data_dir, dataset), exist_ok=True)\n",
    "\n",
    "    # Download if missing\n",
    "    if not os.path.exists(train_path):\n",
    "        urlretrieve(URLS[dataset][\"train\"], train_path)\n",
    "    if not os.path.exists(val_path):\n",
    "        urlretrieve(URLS[dataset][\"val\"], val_path)\n",
    "\n",
    "    with urlopen(URLS[dataset][\"train\"]) as f:\n",
    "        x_train = np.loadtxt(f, dtype=int, delimiter=\",\")\n",
    "\n",
    "    with urlopen(URLS[dataset][\"val\"]) as f:\n",
    "        x_val = np.loadtxt(f, dtype=int, delimiter=\",\")\n",
    "\n",
    "    x_train = torch.from_numpy(x_train)\n",
    "    x_val = torch.from_numpy(x_val)\n",
    "\n",
    "    if max_samples is not None:\n",
    "        x_train = x_train[:max_samples]\n",
    "\n",
    "    D = x_train.shape[1]\n",
    "    cols = torch.randperm(D)\n",
    "    if conditional:\n",
    "        x_train, y_train = x_train[:, cols[: D // 2]], x_train[:, cols[D // 2 :]]\n",
    "        x_val, y_val = x_val[:, cols[: D // 2]], x_val[:, cols[D // 2 :]]\n",
    "    else:\n",
    "        y_train = x_train.clone()\n",
    "        y_val = x_val.clone()\n",
    "        x_train = torch.ones(x_train.shape[0], 1, device=x_train.device).float()\n",
    "        x_val = torch.ones(x_val.shape[0], 1, device=x_val.device).float()\n",
    "\n",
    "    train_set = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    val_set = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "\n",
    "    if batch_size is None:\n",
    "        batch_size = len(train_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35a29582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rows = []\n",
    "for dataset in [\"nltcs\", \"msnbc\", \"kdd\", \"plants\", \"baudio\", \"jester\", \"bnetflix\", \"accidents\", \"pumsb_star\", \"dna\", \"kosarek\", \"msweb\", \"book\"]:\n",
    "    train_dl, test_dl = get_data_loaders(dataset=dataset, conditional=False, max_samples=1000)\n",
    "    _, x_train = next(iter(train_dl))\n",
    "    sparsity = (x_train == 0).sum() / x_train.numel()\n",
    "    avg_mi = avg_pairwise_mi(x_train)\n",
    "    rows.append({\"dataset\": dataset, \"sparsity\": sparsity.item(), \"avg_mi\": avg_mi})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc6573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>avg_mi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>msweb</td>\n",
       "      <td>0.989742</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kosarek</td>\n",
       "      <td>0.979116</td>\n",
       "      <td>0.001334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dna</td>\n",
       "      <td>0.746256</td>\n",
       "      <td>0.001662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>book</td>\n",
       "      <td>0.983548</td>\n",
       "      <td>0.001904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kdd</td>\n",
       "      <td>0.989266</td>\n",
       "      <td>0.002396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>msnbc</td>\n",
       "      <td>0.834412</td>\n",
       "      <td>0.003472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>accidents</td>\n",
       "      <td>0.709360</td>\n",
       "      <td>0.005766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bnetflix</td>\n",
       "      <td>0.461480</td>\n",
       "      <td>0.006361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baudio</td>\n",
       "      <td>0.802590</td>\n",
       "      <td>0.011658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jester</td>\n",
       "      <td>0.379240</td>\n",
       "      <td>0.016603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pumsb_star</td>\n",
       "      <td>0.729294</td>\n",
       "      <td>0.043079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plants</td>\n",
       "      <td>0.807203</td>\n",
       "      <td>0.065820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nltcs</td>\n",
       "      <td>0.681938</td>\n",
       "      <td>0.078530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset  sparsity    avg_mi\n",
       "11       msweb  0.989742  0.000136\n",
       "10     kosarek  0.979116  0.001334\n",
       "9          dna  0.746256  0.001662\n",
       "12        book  0.983548  0.001904\n",
       "2          kdd  0.989266  0.002396\n",
       "1        msnbc  0.834412  0.003472\n",
       "7    accidents  0.709360  0.005766\n",
       "6     bnetflix  0.461480  0.006361\n",
       "4       baudio  0.802590  0.011658\n",
       "5       jester  0.379240  0.016603\n",
       "8   pumsb_star  0.729294  0.043079\n",
       "3       plants  0.807203  0.065820\n",
       "0        nltcs  0.681938  0.078530"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df.sort_values(\"avg_mi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
