{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def linear_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, apply_softmax: bool = False) -> torch.Tensor:\n",
    "    \"\"\"Linear attention\n",
    "\n",
    "    Args:\n",
    "        q (torch.Tensor): (B, T, D)\n",
    "        k (torch.Tensor): (B, T, D)\n",
    "        v (torch.Tensor): (B, T, D)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: (B, T, T)\n",
    "    \"\"\"\n",
    "\n",
    "    # attn[i, j] = <q_i, k_j>\n",
    "    attn = torch.einsum(\"bid,bjd->bij\", q, k)\n",
    "    if apply_softmax:\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "    # y[i] = Σⱼ attn[i, j] * v[j]\n",
    "    y = torch.einsum(\"bij,bjd->bid\", attn, v)\n",
    "    return y\n",
    "\n",
    "\n",
    "class AttnMach(torch.nn.Module):\n",
    "    def __init__(self, n_vars, d_vocab, d_hidden):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.d_vocab = d_vocab\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        # Initialize weights\n",
    "        std_fan_in = torch.sqrt(torch.tensor(2.0)) / d_vocab**0.5\n",
    "        self._w_v = torch.nn.Parameter(torch.randn(1, d_vocab) * std_fan_in)\n",
    "        self._w_q = torch.nn.Parameter(torch.randn(d_hidden, d_vocab) * std_fan_in)\n",
    "        self._w_k = torch.nn.Parameter(torch.randn(d_hidden, d_vocab) * std_fan_in)\n",
    "\n",
    "    @property\n",
    "    def w_v(self) -> torch.Tensor:\n",
    "        return self._w_v.abs()\n",
    "\n",
    "    @property\n",
    "    def w_q(self) -> torch.Tensor:\n",
    "        return self._w_q.abs()\n",
    "\n",
    "    @property\n",
    "    def w_k(self) -> torch.Tensor:\n",
    "        return self._w_k.abs()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Computes log prob of seqs.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): (B, n_vars, d_vocab)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (B,)\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        assert torch.all(x < self.d_vocab), f\"Expected input to be in range [0, {self.d_vocab}), got max {x.max()}\"\n",
    "        x = torch.nn.functional.one_hot(x, num_classes=self.d_vocab).to(torch.get_default_dtype())  # (B, n_vars, d_vocab)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # Compute p_tilde\n",
    "        q = torch.einsum(\"btd,hd->bth\", x, self.w_q)\n",
    "        k = torch.einsum(\"btd,hd->bth\", x, self.w_k)\n",
    "        v = torch.einsum(\"btd,hd->bth\", x, self.w_v)\n",
    "        p_tilde = linear_attention(q, k, v)  # (B, N, d_value)\n",
    "        p_tilde = p_tilde[:, :, 0].sum(dim=-1)  # (B,)\n",
    "\n",
    "        # Compute z_tilde\n",
    "        z_tilde = torch.einsum(\"hq,hk,pk->\", self.w_q, self.w_k, self.w_v)  # (1,)\n",
    "\n",
    "        # Check p_tilde always positive\n",
    "        assert torch.all(p_tilde > 0), \"p_tilde is not always positive\"\n",
    "\n",
    "        # Compute loss\n",
    "        loss = (N-2)*math.log(D) + 2*math.log(N) + z_tilde.log() - p_tilde.log()\n",
    "\n",
    "        return loss.mean()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd52071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vocab = 2\n",
    "d_hidden = 8\n",
    "n_vars = 4\n",
    "\n",
    "x = torch.randint(0, d_vocab, (1, n_vars))\n",
    "\n",
    "# Given discrete r.v.s will fit a density to it\n",
    "model = AttnMach(n_vars, d_vocab, d_hidden)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "for i in range(10_000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if loss.item() < 0:\n",
    "        raise ValueError(\"Loss is negative\")\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i} loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "bfa26936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] linear_attention == scaled_dot_product_attention\n"
     ]
    }
   ],
   "source": [
    "def test_linear_attention():\n",
    "    B, T, D = 1, 4, 8\n",
    "    q, k, v = torch.randn(B, T, D).abs(), torch.randn(B, T, D).abs(), torch.randn(B, T, D).abs()\n",
    "    y_lin = lin_attn = linear_attention(q, k, v, apply_softmax=True)\n",
    "    y_pt = pt_attn = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False, scale=1.0)\n",
    "    return (y_lin == y_pt).all().item()\n",
    "\n",
    "test_res_str = \"PASS\" if test_linear_attention() else \"FAIL\"\n",
    "print(f\"[{test_res_str}] linear_attention == scaled_dot_product_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "fd875eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.772588722239781"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random uniform distribution loss\n",
    "n_vars*math.log(d_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
