{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "113b6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttnMach(torch.nn.Module):\n",
    "    def __init__(self, n_vars, d_vocab, d_hidden):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.d_vocab = d_vocab\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        self._w_v = torch.nn.Parameter(torch.randn(1, d_vocab))\n",
    "        self._w_q = torch.nn.Parameter(torch.randn(d_hidden, d_vocab))\n",
    "        self._w_k = torch.nn.Parameter(torch.randn(d_hidden, d_vocab))\n",
    "\n",
    "    @property\n",
    "    def w_v(self) -> torch.Tensor:\n",
    "        return self._w_v.abs()\n",
    "\n",
    "    @property\n",
    "    def w_q(self) -> torch.Tensor:\n",
    "        return self._w_q.abs()\n",
    "\n",
    "    @property\n",
    "    def w_k(self) -> torch.Tensor:\n",
    "        return self._w_k.abs()\n",
    "\n",
    "    def _contract(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        q = torch.einsum(\"bvd,hd->bvh\", x, self.w_q)\n",
    "        k = torch.einsum(\"bvd,hd->bvh\", x, self.w_k)\n",
    "        v = torch.einsum(\"bvd,hd->bvh\", x, self.w_v)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Computes log prob of seqs.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): (B, n_vars, d_vocab)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (B,)\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        assert torch.all(x < self.d_vocab), f\"Expected input to be in range [0, {self.d_vocab}), got max {x.max()}\"\n",
    "        x = torch.nn.functional.one_hot(x, num_classes=self.d_vocab).to(torch.get_default_dtype())  # (B, n_vars, d_vocab)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # Compute p_tilde\n",
    "        q = torch.einsum(\"bvd,hd->bvh\", x, self.w_q)\n",
    "        k = torch.einsum(\"bvd,hd->bvh\", x, self.w_k)\n",
    "        v = torch.einsum(\"bvd,hd->bvh\", x, self.w_v)\n",
    "        p_tilde = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "        # Compute z_tilde\n",
    "        z_tilde = torch.einsum(\"hq,hk,pk->\", self.w_q, self.w_k, self.w_v)\n",
    "\n",
    "        # Check p_tilde always positive\n",
    "        assert torch.all(p_tilde > 0), \"p_tilde is not always positive\"\n",
    "\n",
    "        # Compute loss\n",
    "        loss = (N-2)*math.log(D) + 2*math.log(N) + z_tilde.log() - p_tilde.squeeze(-1).sum(dim=-1).log()\n",
    "\n",
    "        return loss.mean()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fd52071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss: 5.358197212219238\n",
      "Iteration 100 loss: 5.008886337280273\n",
      "Iteration 200 loss: 4.442053318023682\n",
      "Iteration 300 loss: 3.292545795440674\n",
      "Iteration 400 loss: 0.03184963017702103\n",
      "Iteration 500 loss: -2.3071117401123047\n",
      "Iteration 600 loss: -2.5536210536956787\n",
      "Iteration 700 loss: -1.5034343004226685\n",
      "Iteration 800 loss: -4.55702018737793\n",
      "Iteration 900 loss: -7.7984619140625\n"
     ]
    }
   ],
   "source": [
    "d_vocab = 2\n",
    "d_hidden = 8\n",
    "n_vars = 4\n",
    "\n",
    "x = torch.randint(0, d_vocab, (1, n_vars))\n",
    "\n",
    "# Given discrete r.v.s will fit a density to it\n",
    "model = AttnMach(n_vars, d_vocab, d_hidden)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i} loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e8b0d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
