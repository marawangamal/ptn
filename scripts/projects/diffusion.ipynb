{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b585d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:11<00:00, 14.2MB/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import certifi\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566dd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiffusionConfig:\n",
    "    num_timesteps: int = 1000\n",
    "    num_parallel_steps: int = 10\n",
    "    beta_start: float = 0.0001\n",
    "    beta_end: float = 0.02\n",
    "    beta_schedule: str = \"linear\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiffusionModelOutput:\n",
    "    loss: Optional[torch.Tensor] = None\n",
    "\n",
    "\n",
    "class DiffusionModel(torch.nn.Module):\n",
    "    def __init__(self, config: DiffusionConfig):\n",
    "        self.config = config\n",
    "\n",
    "        self._batch_forward_diffusion = torch.vmap(\n",
    "            self._forward_diffusion,\n",
    "            in_dims=(0, 0)\n",
    "        )\n",
    "        self._batch_reverse_diffusion = torch.vmap(\n",
    "            self._reverse_diffusion,\n",
    "            in_dims=(0, 0)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _forward_diffusion(x0, t):\n",
    "        \"\"\"Forward diffusion process.\n",
    "\n",
    "        Args:\n",
    "            x0 (torch.Tensor): Input images. Shape: (B, C, H, W).\n",
    "            t (torch.Tensor): Timesteps. Shape: (B,).\n",
    "        \"\"\"\n",
    "        # add noise to obtain x_t\n",
    "        pass\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _reverse_diffusion(xT, t):\n",
    "        \"\"\"Decoder (UNet)\n",
    "\n",
    "        Args:\n",
    "            xt (torch.Tensor): Noisy images. Shape: (T, B, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted images. Shape: (T, B, C, H, W).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: Optional[torch.Tensor] = None, num_steps: int = 1000):\n",
    "        \"\"\"Forward pass of the diffusion model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input images. Shape: (B, C, H, W).\n",
    "            y (torch.Tensor): Labels. Shape: (B,).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make inputs\n",
    "        T = self.config.num_timesteps\n",
    "        xt = x.unsqueeze(0).repeat(T, 1, 1, 1, 1)  \n",
    "        t = torch.randint(0, T, (x.shape[0],), device=x.device)\n",
    "        xt = self._batch_forward_diffusion(xt, t) # (T, B, C, H, W)\n",
    "\n",
    "\n",
    "        # Make targets\n",
    "        tm1 = t - 1\n",
    "        xtm1 = self._batch_forward_diffusion(xt, tm1) # (T, B, C, H, W)\n",
    "\n",
    "\n",
    "        # Get predictions\n",
    "        xt_hat = self._batch_reverse_diffusion(self.pos_embed(xtm1), tm1)  \n",
    "\n",
    "        return DiffusionModelOutput(loss=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
