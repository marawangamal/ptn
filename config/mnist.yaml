# Experiment 1
# Evaluating efficacy of multi-token regularization during pretraining and finetuning
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:rtx8000:1"
    - "#SBATCH --mem=32G"
    - "#SBATCH --cpus-per-task=8"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/ctn/.venv/bin/activate"

  glongTamia:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --gres=gpu:4"
    - "#SBATCH --mem=32G"
    - "source /scratch/m/mgamal/ctn/.venv/bin/activate"

group:
  name: "main"
  type: parallel
  jobs:

    - group:
        name: "ctn"
        type: sweep
        preamble: glong
        sweep:
          # so far tried from 1e-3 --  4e-3, 8e-3
          lr: [1e-3, 5e-4, 1e-4]
          rank: [8, 32]
          model: ["bmnc"]
          pos_func: ["exp"]
          mode: ["multitask"]
          scale: [28]
        sweep_template: "python scripts/train_mnist.py --lr {lr} --rank {rank} --model {model} --pos_func {pos_func} --scale {scale} --mode {mode} --epochs 50"

    # - group:
    # mmps_e50_l0004_b32_r8_pexp_irandn_l00_n10_mNone_sFalse
    #     name: "ctn::mnist::mps"
    #     type: sweep
    #     preamble: glong
    #     sweep:
    #       lr: [1e-3]
    #       rank: [8]
    #       model: ["mps"]
    #       pos_func: ["exp"]
    #       lambda_ortho: [0.0]
    #       init_method: ["ortho", "randn"]
    #     sweep_template: "python scripts/train_mnist.py --lr {lr} --rank {rank} --model {model} --pos_func {pos_func} --epochs 50 --tags hpo ortho --lambda_ortho {lambda_ortho} --init_method {init_method}"




